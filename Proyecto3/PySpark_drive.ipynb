{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing using Pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuración en google colab de spark y pyspark\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instalar java y spark\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
    "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load csv Dataset \n",
    "#df=spark.read.csv('s3://<bucket/dir>/sample_data.csv',inferSchema=True,header=True)\n",
    "df=spark.read.csv('/content/gdrive/MyDrive/st0263-2266/bigdata/datasets/covid19/Casos_positivos_de_COVID-19_en_Colombia-100K.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns of dataframe\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datatypes\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select only 5 columns\n",
    "df.select('Nombre departamento','Edad', 'Sexo','Estado', 'Fecha de diagnóstico').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change column name\n",
    "df.select('Nombre Municipio', 'Edad').withColumnRenamed('Nombre Municipio', 'Mup').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add a column\n",
    "df.select('ID de caso', 'Fecha de recuperación', 'Edad').withColumn(\"Edad luego 20 años\",(df[\"Edad\"]+20)).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop columns\n",
    "df.select('fecha reporte web', 'ID de caso', 'Fecha de notificación', 'Nombre departamento').drop('fecha reporte web', 'fecha de notificación').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter the records \n",
    "df.filter(df['Nombre departamento']=='BOGOTA').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "#using lambda function\n",
    "age_udf = udf(lambda age: \"Adulto\" if age <= 30 else \"Joven\", StringType())\n",
    "#apply udf on dataframe\n",
    "df.withColumn(\"Grupo de edad\", age_udf(df.Edad)).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More covid victims in DEPS\n",
    "df.groupBy('Nombre departamento').count().orderBy('count',ascending=False).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More important MUNS\n",
    "df.groupBy('Nombre municipio').count().orderBy('count',ascending=False).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More covid victims per Date\n",
    "\n",
    "df.groupBy('Fecha de notificación').count().orderBy('count',ascending=False).show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cases of covid per age\n",
    "\n",
    "df.groupBy('Edad').count().orderBy('Edad',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cases of covid per country\n",
    "\n",
    "df.groupBy('Nombre del país').count().orderBy('Nombre del país',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Python for SparkSQL\n",
    "from pyspark.sql import SparkSession        \n",
    "# Create a SparkSession\n",
    "spark = (SparkSession\n",
    "  .builder\n",
    "  .appName(\"SparkSQLExampleApp\")\n",
    "  .getOrCreate())\n",
    "\n",
    "# Path to data set\n",
    "csv_file = \"/content/gdrive/MyDrive/st0263-2266/bigdata/datasets/covid19/Casos_positivos_de_COVID-19_en_Colombia-100K.csv\"\n",
    "\n",
    "# Read and create a temporary view\n",
    "# Infer schema (note that for larger files you \n",
    "# may want to specify the schema)\n",
    "df = (spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(csv_file))\n",
    "df.createOrReplaceTempView(\"covid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More covid victims in DEPS\n",
    "spark.sql(\"\"\"SELECT `Nombre departamento`, COUNT(*) FROM covid GROUP BY `Nombre departamento` ORDER BY 2  DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More important MUNS\n",
    "spark.sql(\"\"\"SELECT `Nombre municipio`, COUNT(*) FROM covid GROUP BY `Nombre municipio` ORDER BY 2  DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More covid victims per Date\n",
    "spark.sql(\"\"\"SELECT `Fecha de notificación`, COUNT(*) FROM covid GROUP BY `Fecha de notificación` ORDER BY 2  DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cases of covid per age\n",
    "spark.sql(\"\"\"SELECT `Edad`, COUNT(*) FROM covid GROUP BY `Edad` ORDER BY 1 ASC\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cases of covid per country\n",
    "spark.sql(\"\"\"SELECT `Nombre del país`, COUNT(*) FROM covid GROUP BY `Nombre del país` ORDER BY 2  DESC\"\"\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "c8f579d564c37de2db8317594960e767cd86093094999d1d828ece58aa180931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
